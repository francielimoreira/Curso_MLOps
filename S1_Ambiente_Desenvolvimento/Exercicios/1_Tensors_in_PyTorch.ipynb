{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introdução ao Deep Learning com PyTorch\n",
    "\n",
    "Neste caderno, você terá uma introdução ao [PyTorch](http://pytorch.org/), que é uma estrutura para construção e treinamento de redes neurais (NN). ``PyTorch`` se comporta de várias maneiras como os arrays que você conhece e adora do Numpy. Afinal, esses arrays Numpy são apenas *tensores*. PyTorch pega esses tensores e simplifica movê-los para GPUs para o processamento mais rápido necessário ao treinar redes neurais. Ele também fornece um módulo que calcula gradientes automaticamente (para retropropagação!) E outro módulo específico para construção de redes neurais. No geral, o PyTorch acaba sendo mais coerente com o **Python** e a pilha ``Numpy/Scipy`` em comparação com o *TensorFlow* e outros frameworks.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redes neurais\n",
    "\n",
    "O Deep Learning é baseado em [redes neurais artificiais](https://en.wikipedia.org/wiki/Artificial_neural_network) que existem de alguma forma desde o final dos anos 1950. As redes são construídas a partir de partes individuais que se aproximam dos neurônios, normalmente chamadas de unidades ou simplesmente “neurônios”. Cada unidade possui um certo número de entradas ponderadas. Essas entradas ponderadas são somadas (uma combinação linear) e depois passadas por uma função de ativação para obter a saída da unidade.\n",
    "\n",
    "<img src=\"assets/simple_neuron.png\" largura=400px>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Matematicamente isso se parece com:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\\n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Com vetores, este é o produto escalar/interno de dois vetores:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Tensores\n",
    "\n",
    "Acontece que os cálculos de redes neurais são apenas um monte de operações de álgebra linear em *tensores*, uma generalização de matrizes. Um vetor é um tensor unidimensional, uma matriz é um tensor bidimensional, uma matriz com três índices é um tensor tridimensional (imagens coloridas RGB, por exemplo). A estrutura de dados fundamental para redes neurais são tensores e PyTorch (assim como praticamente todas as outras estruturas de aprendizado profundo) é construído em torno de tensores.\n",
    "\n",
    "<img src=\"assets/tensor_examples.svg\" width=600px>\n",
    "\n",
    "Com o básico abordado, é hora de explorar como podemos usar o PyTorch para construir uma rede neural simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch.\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data.\n",
    "torch.manual_seed(7) #Define a semente do gerador de números aleatórios para garantir reprodutibilidade nos resultados.\n",
    "\n",
    "#Gera uma matriz de dimensão 1×5 contendo valores aleatórios de uma distribuição normal \n",
    "features = torch.randn((1, 5))\n",
    "# Gera uma matriz de pesos aleatórios com as mesmas dimensões que as características. \n",
    "#Esta matriz será usada para ponderar os recursos na regressão linear.\n",
    "weights = torch.randn_like(features)\n",
    "# and a true bias term.\n",
    "bias = torch.randn((1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acima, gerei dados que podemos usar para obter a saída de nossa rede simples. Isso tudo é aleatório por enquanto, daqui para frente começaremos a usar dados normais. Percorrendo cada linha relevante:\n",
    "\n",
    "`features = torch.randn((1, 5))` cria um tensor com forma `(1, 5)`, uma linha e cinco colunas, que contém valores distribuídos aleatoriamente de acordo com a distribuição normal com média zero e padrão desvio de um.\n",
    "\n",
    "`weights = torch.randn_like(features)` cria outro tensor com a mesma forma de `features`, novamente contendo valores de uma distribuição normal.\n",
    "\n",
    "Finalmente, `bias = torch.randn((1, 1))` cria um único valor de uma distribuição normal.\n",
    "\n",
    "Os tensores PyTorch podem ser adicionados, multiplicados, subtraídos, etc., assim como os arrays Numpy. Em geral, você usará tensores PyTorch praticamente da mesma forma que usaria matrizes Numpy. Eles vêm com alguns benefícios interessantes, como aceleração de GPU, que veremos mais tarde. Por enquanto, use os dados gerados para calcular a saída desta rede simples de camada única.\n",
    "> **Exercício**: Calcule a saída da rede com recursos de entrada `recursos`, pesos `pesos` e polarização `viés`. Semelhante ao Numpy, o PyTorch tem uma função [`torch.sum()`](https://pytorch.org/docs/stable/torch.html#torch.sum), bem como um método `.sum()` em tensores, para obter somas. Use a função `ativação` definida acima como a função de ativação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of this network using the weights and bias tensors.\n",
    "\n",
    "# Calcula a saída da rede neural\n",
    "output = activation(torch.sum(features * weights) + bias)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode fazer a multiplicação e a soma na mesma operação usando uma multiplicação de matrizes. Em geral, você desejará usar multiplicações de matrizes, pois elas são mais eficientes e aceleradas usando bibliotecas modernas e computação de alto desempenho em GPUs.\n",
    "\n",
    "Aqui, queremos fazer uma multiplicação matricial dos features and the weights. Para isso podemos usar [`torch.mm()`](https://pytorch.org/docs/stable/torch.html#torch.mm) ou [`torch.matmul()`](https:// pytorch.org/docs/stable/torch.html#torch.matmul), que é um pouco mais complicado e suporta transmissão. Se tentarmos fazer isso com `features` e `weights` como estão, obteremos um erro\n",
    "\n",
    "```python\n",
    ">> torch.mm(features, weights)\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "RuntimeError                              Traceback (most recent call last)\n",
    "<ipython-input-13-15d592eb5279> in <module>()\n",
    "----> 1 torch.mm(features, weights)\n",
    "\n",
    "RuntimeError: size mismatch, m1: [1 x 5], m2: [1 x 5] at /Users/soumith/minicondabuild3/conda-bld/pytorch_1524590658547/work/aten/src/TH/generic/THTensorMath.c:2033\n",
    "```\n",
    "\n",
    "Ao construir redes neurais em qualquer estrutura, você verá isso com frequência. Muitas vezes. O que está acontecendo aqui é que nossos tensores não têm a forma correta para realizar uma multiplicação de matrizes. Lembre-se que para multiplicações de matrizes, o número de colunas no primeiro tensor deve ser igual ao número de linhas no segundo tensor. Ambos `features` e `weight` têm a mesma forma, `(1, 5)`. Isso significa que precisamos mudar a forma dos `weight` para que a multiplicação da matriz funcione.\n",
    "\n",
    "**Nota:** Para ver a forma de um tensor chamado `tensor`, use `tensor.shape`. Se você estiver construindo redes neurais, usará esse método com frequência.\n",
    "\n",
    "Existem algumas opções aqui: [`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape), [`weights.resize_()`]( https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_), [`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch .Tensor.view) e [`torch.transpose(weights,0,1)`](https://pytorch.org/docs/master/generated/torch.transpose.html).\n",
    "\n",
    "* `weights.reshape(a, b)` retornará um novo tensor com os mesmos dados que `weights` com tamanho `(a, b)` às vezes, e às vezes um clone, já que copia os dados para outra parte do memória.\n",
    "* `weights.resize_(a, b)` retorna o mesmo tensor com uma forma diferente. Porém, se a nova forma resultar em menos elementos que o tensor original, alguns elementos serão removidos do tensor (mas não da memória). Se a nova forma resultar em mais elementos do que o tensor original, os novos elementos não serão inicializados na memória. Aqui devo observar que o sublinhado no final do método indica que este método é executado **no local**. Aqui está um ótimo tópico no fórum para [ler mais sobre operações no local](https://discuss.pytorch.org/t/what-is-in-place-operation/16244) no PyTorch.\n",
    "* `weights.view(a, b)` retornará um novo tensor com os mesmos dados que `weights` com tamanho `(a, b)`.\n",
    "* `torch.transpose(weights,0,1)` retornará o tensor de pesos transpostos. Isso retorna a versão transposta do tensor inpjut ao longo de dim 0 e dim 1. Isso é eficiente, pois não especificamos as dimensões reais dos pesos.\n",
    "\n",
    "Eu costumo usar `.view()`, mas qualquer um dos três métodos funcionará para isso. Então, agora podemos remodelar `weights` para ter cinco linhas e uma coluna com algo como `weights.view(5, 1)`.\n",
    "\n",
    "Mais uma abordagem é usar `.t()` para transpor o vetor de pesos, no nosso caso da forma (1,5) para (5,1).\n",
    "> **Exercício**: Calcule a saída de nossa pequena rede usando multiplicação de matrizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "## Calculate the output of this network using matrix multiplication\n",
    "weights = weights.view(5, 1)\n",
    "\n",
    "output = activation(torch.mm(features, weights) + bias)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Empilhe-os!\n",
    "\n",
    "É assim que você pode calcular a saída de um único neurônio. O verdadeiro poder desse algoritmo acontece quando você começa a empilhar essas unidades individuais em camadas e pilhas de camadas, em uma rede de neurônios. A saída de uma camada de neurônios torna-se a entrada da próxima camada. Com múltiplas unidades de entrada e unidades de saída, agora precisamos expressar os pesos como uma matriz.\n",
    "\n",
    "<img src='assets/multilayer_diagram_weights.png' largura=450px>\n",
    "\n",
    "A primeira camada mostrada abaixo são as entradas, compreensivelmente chamadas de **camada de entrada**. A camada intermediária é chamada de **camada oculta**, e a camada final (à direita) é a **camada de saída**. Podemos expressar esta rede matematicamente com matrizes novamente e usar a multiplicação de matrizes para obter combinações lineares para cada unidade em uma operação. Por exemplo, a camada oculta ($h_1$ e $h_2$ aqui) pode ser calculada\n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "A saída para esta pequena rede é encontrada tratando a camada oculta como entradas para a unidade de saída. A saída da rede é expressa simplesmente\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Generate some data.\n",
    "\n",
    "torch.manual_seed(7)  # Define a semente aleatória para garantir a previsibilidade.\n",
    "\n",
    "# Os recursos são 3 variáveis normais aleatórias.\n",
    "features = torch.randn((1, 3))\n",
    "\n",
    "# Define o tamanho de cada camada em nossa rede.\n",
    "n_input = features.shape[1]     # Número de unidades de entrada, deve coincidir com o número de recursos de entrada.\n",
    "n_hidden = 2                    # Número de unidades ocultas.\n",
    "n_output = 1                    # Número de unidades de saída.\n",
    "\n",
    "# Pesos para as entradas para a camada oculta.\n",
    "W1 = torch.randn(n_input, n_hidden)\n",
    "# Pesos para a camada oculta para a camada de saída.\n",
    "W2 = torch.randn(n_hidden, n_output)\n",
    "\n",
    "# and bias terms para as camadas oculta e de saída\n",
    "B1 = torch.randn((1, n_hidden))\n",
    "B2 = torch.randn((1, n_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercício:** Calcule a saída para esta rede multicamadas usando os pesos `W1` e `W2` e as tendências, `B1` e `B2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "## Your solution here.\n",
    "# Calcula a saída da camada oculta\n",
    "hidden_output = activation(torch.mm(features, W1) + B1)\n",
    "\n",
    "# Calcula a saída da camada de saída\n",
    "output = activation(torch.mm(hidden_output, W2) + B2)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se você fez isso corretamente, deverá ver a saída `tensor([[ 0.3171]])`.\n",
    "\n",
    "O número de unidades ocultas é um parâmetro da rede, geralmente chamado de **hiperparâmetro** para diferenciá-lo dos parâmetros de pesos e bias. Como você verá mais tarde, quando discutirmos o treinamento de uma rede neural, quanto mais unidades ocultas uma rede tiver, e quanto mais camadas, mais capaz ela será de aprender com os dados e fazer previsões precisas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numpy para torch e vice-versa\n",
    "\n",
    "Seção de bônus especial! PyTorch tem um ótimo recurso para conversão entre arrays Numpy e tensores Torch. Para criar um tensor a partir de um array Numpy, use `torch.from_numpy()`. Para converter um tensor em um array Numpy, use o método `.numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77972024, 0.92298773, 0.77081473],\n",
       "       [0.63043875, 0.39674868, 0.07300527],\n",
       "       [0.22249923, 0.24156571, 0.79578098],\n",
       "       [0.09626836, 0.45104908, 0.51498427]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=8)\n",
    "a = np.random.rand(4,3)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.77972024, 0.92298773, 0.77081473],\n",
       "        [0.63043875, 0.39674868, 0.07300527],\n",
       "        [0.22249923, 0.24156571, 0.79578098],\n",
       "        [0.09626836, 0.45104908, 0.51498427]], dtype=torch.float64)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_printoptions(precision=8)\n",
    "b = torch.from_numpy(a)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.61995741, 0.33155467, 0.8174536 ],\n",
       "       [0.34563545, 0.15472967, 0.23112194],\n",
       "       [0.57911665, 0.7242849 , 0.41691137],\n",
       "       [0.68045155, 0.49653628, 0.32549209]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A memória é compartilhada entre o array Numpy e o tensor Torch, portanto, se você alterar os valores no local de um objeto, o outro também mudará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.55944048, 1.84597547, 1.54162946],\n",
       "        [1.26087751, 0.79349736, 0.14601054],\n",
       "        [0.44499845, 0.48313142, 1.59156196],\n",
       "        [0.19253673, 0.90209817, 1.02996854]], dtype=torch.float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply PyTorch Tensor by 2, in place.\n",
    "b.mul_(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.55944048, 1.84597547, 1.54162946],\n",
       "       [1.26087751, 0.79349736, 0.14601054],\n",
       "       [0.44499845, 0.48313142, 1.59156196],\n",
       "       [0.19253673, 0.90209817, 1.02996854]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Numpy array matches new values from Tensor.\n",
    "a\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "fec304111c2370d16a75024b04fb2eaaf4ef35bad1f23d41084af01dd381d109"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
